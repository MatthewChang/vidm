<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0069)http://www.cs.cmu.edu/~dchaplot/projects/neural-topological-slam.html -->

<html xmlns="http://www.w3.org/1999/xhtml">
<!-- ======================================================================= -->

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii" />
  <script type="text/javascript" id="www-widgetapi-script" src="./website_files/www-widgetapi.js" async="">
</script>
  <script src="./website_files/jsapi" type="text/javascript">
</script>
  <script type="text/javascript">
//<![CDATA[
  google.load("jquery", "1.3.2");
  //]]>
  </script>
  <script type="text/javascript" charset="UTF-8" src="./website_files/jquery.min.js">
</script>
  <style type="text/css">
/*<![CDATA[*/
  body {
    font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:18px;
    margin-left: auto;
    margin-right: auto;
    width: 1100px;
  }

  h1 {
    font-weight:300;
  }

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }

  #authors td {
    padding-bottom:5px;
    padding-top:30px;
  }
  /*]]>*/
  </style><!-- ======================================================================= -->
  <!-- Global site tag (gtag.js) - Google Analytics -->

  <script async="" src="./website_files/js" type="text/javascript">
</script>
  <script type="text/javascript">
//<![CDATA[
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-114291442-6');
  //]]>
  </script>
  <script type="text/javascript" src="./website_files/hidebib.js">
</script>
  <link href="./website_files/css" rel="stylesheet" type="text/css" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link rel="icon" type="image/png" href="" />

  <title>Semantic Visual Navigation by Watching Youtube Videos</title>
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="./website_files/iframe_api" type="text/javascript">
</script>
</head>

<body>
  <br />


  <center>
      <span style="font-size:44px;font-weight:bold;">Look Ma, No Hands! </br>Agent-Environment Factorization</br> of Egocentric Videos
</span>
  </center>
  <br />


  <table align="center" width="800px">
    <tbody>
      <tr>
        <td align="center" width="230px">
          <center>
            <span style="font-size:22px">Matthew Chang</span>
          </center>
        </td>

        <td align="center" width="230px">
          <center>
            <span style="font-size:22px">Aditya Prakash</span>
          </center>
        </td>

        <td align="center" width="230px">
          <center>
            <span style="font-size:22px">Saurabh Gupta</span>
          </center>
        </td>
      </tr>


      <tr>
        <td align="center" width="230">
          <center>
            <span style="font-size:20px">UIUC</span>
          </center>
        </td>

        <td align="center" width="230">
          <center>
            <span style="font-size:20px">UIUC</span>
          </center>
        </td>

        <td align="center" width="230px">
          <center>
            <span style="font-size:20px">UIUC</span>
          </center>
        </td>
      </tr>
    </tbody>
  </table>
  <!--<table align="center" width="700px" style='margin-bottom: 30px'>-->
      <!--<tr>-->
        <!--<td align="center" width="230px">-->
          <!--<center>-->
            <!--<span style="font-size:20px; font-weight: 500;">Appearing in NeurIPS 2020</span>-->
          <!--</center>-->
        <!--</td>-->
      <!--</tr>-->
  <!--</table>-->

  <table align="center" width="700px">
          <tbody><tr>
            <td align="center" width="200px"><center><span style="font-size:24px"><a href="">Paper</a></span></center></td>
            <td align=center width=200px><center><span style="font-size:24px">Code (coming soon)</span></center></td>
          </tr><tr>
      </tr></tbody></table>
  <table align="center" width="300px" style="padding-top: 25px">
    <tbody>
      <tr>
        <td align="center" width="300px"><iframe width="700" height="400" src="https://www.youtube.com/embed/oXsH5gP2ZWs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></iframe>
        </td>
      </tr>
    </tbody>
  </table>
  <br />

  <br />


  <div style="width:800px; margin:0 auto; text-align:justify">The analysis and use of egocentric videos for robotic tasks is made challenging by occlusion due to the hand and the visual mismatch between the human hand and a robot end-effector. In this sense, the human hand presents a nuisance. However, often hands also provide a valuable signal, e.g. the hand pose may suggest what kind of object is being held. In this work, we propose to extract a factored representation of the scene that separates the agent (human hand) and the environment. This alleviates both occlusion and mismatch while preserving the signal, thereby easing the design of models for downstream robotics tasks. At the heart of this factorization is our proposed Video Inpainting via Diffusion Model (VIDM) that leverages both a prior on real-world images (through a large-scale pre-trained diffusion model) and the appearance of the object in earlier frames of the video (through attention). Our experiments demonstrate the effectiveness of VIDM at improving inpainting quality on egocentric videos and the power of our factored representation for numerous tasks: object detection, 3D reconstruction of manipulated objects, and learning of reward functions, policies, and affordances from videos.</div>
  <br />

  <hr />

  <center>
    <h1>Agent-Environment Factorization of Egocentric Videos</h1>
  </center>
  <div style="width:800px; margin:0 auto; text-align:justify">
Occlusion and the visual
mismatch between humans and robots, make it difficult to use egocentric videos for robotic tasks. We
propose a pixel-space factorization of egocentric videos into agent and environment representations. An agent representation is obtained using a model to segment out the agent. The environment representation is obtained by inpainting out the agent from the original image using VIDM, a novel Video Inpainting Diffusion Model.
</div>
  <br />
  <p style="margin-top:4px;">
  </p>
  <table align="center" width="1000px">
    <tbody>
      <tr>
        <td width="1000px">
          <center>
            <img src="./website_files/factorization.png" width="650px" /><br />
          </center>
        </td>
      </tr>
    </tbody>
  </table>
  <br />

  <center>
    <h1>Applications of Agent-Environment Factorizations</h1>
  </center>
  <div style="width:800px; margin:0 auto; text-align:justify">
Agent-Environment Factored (AEF) representations enable many applications. (a) For
visual perception tasks, the unoccluded environment can be used in addition to the original image. 
(b) For affordance learning tasks, the unoccluded environment
can be used to predict relevant desirable aspects of the agent. 
(c) For reward learning tasks agent representations can be transformed into agent-agnostic formats for more
effective transfer across embodiments.</div>
  <br />
  <p style="margin-top:4px;">
  </p>
  <table align="center" width="1000px">
    <tbody>
      <tr>
        <td width="1000px">
          <center>
            <img src="./website_files/application.png" width="800px" /><br />
          </center>
        </td>
      </tr>
    </tbody>
  </table>
  <br />

  <center>
    <h1>Video Inpainting Diffusion Model (VIDM)</h1>
  </center>
  <div style="width:800px; margin:0 auto; text-align:justify">
We extend pre-trained single-frame inpainting diffusion models to videos. Features from context frames are introduced as additional inputs into the Attention Block A. We repeat the multi-frame attention block 8 times (4 to encode and 4 to decode) to construct the U-Net that conducts 1 step of denoising. The U-Net operates in the VQ encoder latent space.
  </div>
  <br />
  <p style="margin-top:4px;">
  </p>
  <table align="center" width="1000px">
    <tbody>
      <tr>
        <td width="1200px">
          <center>
            <img src="./website_files/figure.png" width="800px" /><br />
          </center>
        </td>
      </tr>
    </tbody>
  </table>
  <br />

  <hr />

  <center>
    <h1>Paper</h1>
  </center>


  <table align="center" width="auto">
    <tbody>
      <tr>
        <td width="200px" align="left">
          <a href=""><img style="width:200px;border-style: solid; border-color: black; border-width: thin" src="./website_files/thumbnail.png"/></a>
          <center>
          </center>
        </td>
        <td width="50px" align="center">
        </td>

        <td width="450px" align="left">
          <span style="font-size:6px;">&nbsp;<br /></span> <span style="font-size:15pt">Matthew Chang, Aditya Prakash, Saurabh Gupta. <br/>Look Ma, No Hands! Agent-Environment Factorization of Egocentric Videos. <br/>Arxiv Preprint.</span></p>
          <div class="paper" id="assemblies19_bib">
            <pre xml:space="preserve" style="display: block; font-size: 12px">
@inproceedings{chang2023look,
  title={Look Ma, No Hands! Agent-Environment Factorization of Egocentric Videos},
  author={Matthew Chang and Aditya Prakash and Saurabh Gupta},
  booktitle={Arxiv Preprint},
  year={2023}
}
</pre>
          </div>
        </td>
      </tr>


    </tbody>
  </table>
  <br />

  <hr/>
<table align="center" width="800px">
      <tbody><tr><td width="800px"><left>
      <center><h1>Acknowledgements</h1></center>
We thank Ruisen Tu and Advait Patel for their help with collecting segmentation annotations for the claw and the robot end-effector. We also thank Arjun Gupta, Erin Zhang, Shaowei Liu, and Joseph Lubars for their feedback on the manuscript. This material is based upon work supported by NSF (IIS2007035), DARPA (Machine Common Sense program), NASA (80NSSC21K1030), an Amazon Research Award, an NVidia Academic Hardware Grant, and the NCSA Delta System (supported by NSF OCI 2005572 and the State of Illinois)
          Website template from <a href="https://richzhang.github.io/colorization">here</a>, <a href="https://pathak22.github.io/modular-assemblies/">here</a>, and <a href="http://www.cs.cmu.edu/~dchaplot/projects/neural-topological-slam.html">here</a>. <br>
      </left></td></tr>
    </tbody></table>

  <br />
  <script xml:space="preserve" language="JavaScript" type="text/javascript">
  <!--hideallbibs();-->
  </script>
</body>
</html>
